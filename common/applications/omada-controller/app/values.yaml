---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/common-4.2.0/charts/library/common/values.schema.json
global:
  alwaysAppendIdentifierToResourceName: true

controllers:
  main:
    containers:
      omada-controller:
        image:
          repository: mbentley/omada-controller
          pullPolicy: IfNotPresent
          tag: 6.1.0.19

        env:
          - name: MANAGE_HTTP_PORT
            value: 80
          - name: MANAGE_HTTPS_PORT
            value: 443
          - name: PORTAL_HTTP_PORT
            value: 8088
          - name: PORTAL_HTTPS_PORT
            value: 8843
          - name: MONGO_EXTERNAL
            value: "true"
          - name: EAP_MONGOD_URI
            value: "mongodb://omada-controller-mongodb:27017/omada"

        securityContext:
          capabilities:
            add:
              - CAP_NET_BIND_SERVICE

        resources:
          requests:
            cpu: 100m
            memory: 2875Mi
          limits:
            memory: 2875Mi

        probes:
          liveness: &server_probes
            enabled: true
            custom: true
            spec:
              tcpSocket:
                port: 443
              initialDelaySeconds: 120
              periodSeconds: 60
              timeoutSeconds: 1
              failureThreshold: 7
          readiness: *server_probes
          startup:
            enabled: false

defaultPodOptions:
  terminationGracePeriodSeconds: 60
  securityContext:
    sysctls:
      - name: net.ipv4.ip_unprivileged_port_start
        value: "0"

service:
  omada-controller-http:
    controller: main
    ipFamilyPolicy: PreferDualStack
    ports:
      manage:
        port: 80
      portal:
        port: 8088
  omada-controller-https:
    controller: main
    ipFamilyPolicy: PreferDualStack
    annotations:
      traefik.ingress.kubernetes.io/service.serversscheme: https
    ports:
      manage:
        port: 443
      portal:
        port: 8843
  omada-controller:
    controller: main
    ipFamilyPolicy: PreferDualStack
    type: LoadBalancer
    loadBalancerClass: ${CILIUM_L2_LB_CLASS}
    externalTrafficPolicy: Local
    annotations:
      ${CILIUM_IP_ANNOTATION}: "${OMADA_CONTROLLER_IPV4}"
    labels:
      "loadbalancer/pool": l2
    ports:
      manage:
        port: 443
      app-discovery:
        port: 27001
        protocol: UDP
      discovery:
        port: 29810
        protocol: UDP
      manager-v1:
        port: 29811
      adopt-v1:
        port: 29812
      upgrade-v1:
        port: 29813
      manager-v2:
        port: 29814
      transfer-v2:
        port: 29815
      rtty:
        port: 29816
      device-monitor:
        port: 29817

# TODO: we cannot use the ingress due to https://github.com/mbentley/docker-omada-controller/discussions/317
# hence, we are setting the endpoint to the load balancer and create a new ingress resource using our extensions
# ingress:
#   omada-controller:
#     enabled: true
#     annotations:
#       cert-manager.io/cluster-issuer: "${PUBLIC_CLUSTER_ISSUER}"
#       external-dns.alpha.kubernetes.io/provider: public
#       external-dns.alpha.kubernetes.io/target: "${PUBLIC_IP}"
#       traefik.ingress.kubernetes.io/router.tls: "true"
#     hosts:
#       - host: &domain omada.network.vie.home.muehlbachler.io
#         paths:
#           - path: /
#             pathType: ImplementationSpecific
#             service:
#               identifier: omada-controller-http
#               port: 443
#     tls:
#       - secretName: omada-controller-tls-cert
#         hosts:
#           - *domain

persistence:
  data:
    enabled: true
    type: persistentVolumeClaim
    storageClass: ${DEFAULT_STORAGE_CLASS}
    accessMode: ReadWriteOnce
    size: 5Gi
    globalMounts:
      - path: /opt/tplink/EAPController/data
  logs:
    enabled: true
    type: persistentVolumeClaim
    storageClass: ${DEFAULT_STORAGE_CLASS}
    accessMode: ReadWriteOnce
    size: 256Mi
    globalMounts:
      - path: /opt/tplink/EAPController/logs
